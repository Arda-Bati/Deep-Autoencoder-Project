{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from TIMIT_dataloader_DAE import prepareTIMIT_train\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n"
     ]
    }
   ],
   "source": [
    "num_frames = 11\n",
    "batch_size = 256         # Number of samples in each minibatch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 8, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "train_loader, val_loader = prepareTIMIT_train(batch_size = batch_size, \n",
    "                                              num_frames = num_frames, \n",
    "                                              shuffle = True,\n",
    "                                              seed = 1,\n",
    "                                              extras=extras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_size, \n",
    "                 num_layers = 1, \n",
    "                 tied = True,\n",
    "                 batch_normalization = False):\n",
    "        \n",
    "        super(AE, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.tied = tied\n",
    "        self.batch_normalization = batch_normalization\n",
    "        \n",
    "        self.W1 = nn.Parameter(torch.randn(hidden_size, input_size) * 0.1)\n",
    "        self.W2 = nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
    "        self.b = nn.Parameter(torch.randn(hidden_size) * 0.01)\n",
    "        self.c = nn.Parameter(torch.randn(input_size) * 0.01)\n",
    "        \n",
    "        self.normed = nn.BatchNorm1d(hidden_size)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            batch = F.linear(batch, self.W1, bias = self.b)\n",
    "#             if self.batch_normalization:\n",
    "            batch = self.normed(batch)\n",
    "            batch = F.sigmoid(batch)\n",
    "            # if self.tied:\n",
    "            batch = F.linear(batch, weight = self.W1.t(), bias = self.c)\n",
    "            # else:\n",
    "                # batch = F.linear(batch, weight = self.W2, bias = self.c)\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input size as dimension of features, hidden size is # hidden unit, and number of classes is the dimension of dictionary\n",
    "input_size = num_frames * 65\n",
    "hidden_size = 500\n",
    "num_layers = 3\n",
    "\n",
    "model = AE(input_size = input_size, \n",
    "           hidden_size = hidden_size,\n",
    "           num_layers = num_layers,\n",
    "           tied = True,\n",
    "           batch_normalization = False)\n",
    "model = model.to(computing_device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "criterion = criterion.to(computing_device)\n",
    "\n",
    "criterion_val = nn.MSELoss(reduction='sum')\n",
    "criterion_val = criterion_val.to(computing_device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 0.0002)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, criterion, val_loader):\n",
    "\n",
    "    full_val_loss = 0.0\n",
    "    count = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for minibatch_count, (input, target) in enumerate(val_loader, 0):\n",
    "\n",
    "            batch_size, _  = input.shape\n",
    "            \n",
    "            input = input.float()\n",
    "            target = target.float()\n",
    "            \n",
    "            input = input.to(computing_device)\n",
    "            target = target.to(computing_device)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            full_val_loss += loss\n",
    "            count += batch_size\n",
    "    \n",
    "    avg_loss = full_val_loss / count\n",
    "    scheduler.step(avg_loss)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c32eb734c54a18b28059b1d560b9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=64532), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/03/103/m3liang/.local/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch: 0. Training loss: 4.315248489379883\n",
      "Minibatch: 100. Training loss: 1.5449827909469604\n",
      "Minibatch: 200. Training loss: 1.3817142248153687\n",
      "Minibatch: 300. Training loss: 1.3263052701950073\n",
      "Minibatch: 400. Training loss: 1.4581021070480347\n",
      "Minibatch: 500. Training loss: 1.2081118822097778\n",
      "Minibatch: 600. Training loss: 1.2433403730392456\n",
      "Minibatch: 700. Training loss: 1.1042286157608032\n",
      "Minibatch: 800. Training loss: 1.107130765914917\n",
      "Minibatch: 900. Training loss: 1.0067908763885498\n",
      "Minibatch: 1000. Training loss: 1.1362861394882202\n",
      "Minibatch: 1100. Training loss: 1.0064464807510376\n",
      "Minibatch: 1200. Training loss: 0.8355127573013306\n",
      "Minibatch: 1300. Training loss: 1.069661259651184\n",
      "Minibatch: 1400. Training loss: 0.9805367588996887\n",
      "Minibatch: 1500. Training loss: 0.9639266729354858\n",
      "Minibatch: 1600. Training loss: 0.9421364665031433\n",
      "Minibatch: 1700. Training loss: 0.8836820721626282\n",
      "Minibatch: 1800. Training loss: 0.9240055084228516\n",
      "Minibatch: 1900. Training loss: 0.9239912629127502\n",
      "Minibatch: 2000. Training loss: 0.8639736175537109\n",
      "Minibatch: 2100. Training loss: 0.8433905243873596\n",
      "Minibatch: 2200. Training loss: 0.9277710914611816\n",
      "Minibatch: 2300. Training loss: 0.8642984628677368\n",
      "Minibatch: 2400. Training loss: 0.8587960004806519\n",
      "Minibatch: 2500. Training loss: 0.9703304171562195\n",
      "Minibatch: 2600. Training loss: 0.8632223606109619\n",
      "Minibatch: 2700. Training loss: 0.9032490253448486\n",
      "Minibatch: 2800. Training loss: 0.9285528659820557\n",
      "Minibatch: 2900. Training loss: 0.9945682287216187\n",
      "Minibatch: 3000. Training loss: 0.903164803981781\n",
      "Minibatch: 3100. Training loss: 0.9391665458679199\n",
      "Minibatch: 3200. Training loss: 0.9021103382110596\n",
      "Minibatch: 3300. Training loss: 0.818432092666626\n",
      "Minibatch: 3400. Training loss: 0.9052106142044067\n",
      "Minibatch: 3500. Training loss: 0.8655959963798523\n",
      "Minibatch: 3600. Training loss: 0.9033467173576355\n",
      "Minibatch: 3700. Training loss: 0.843593418598175\n",
      "Minibatch: 3800. Training loss: 0.993725061416626\n",
      "Minibatch: 3900. Training loss: 0.8571528196334839\n",
      "Minibatch: 4000. Training loss: 0.8698506951332092\n",
      "Minibatch: 4100. Training loss: 0.896161675453186\n",
      "Minibatch: 4200. Training loss: 0.8554529547691345\n",
      "Minibatch: 4300. Training loss: 0.9679617285728455\n",
      "Minibatch: 4400. Training loss: 0.8919983506202698\n",
      "Minibatch: 4500. Training loss: 0.8689926266670227\n",
      "Minibatch: 4600. Training loss: 0.8799458742141724\n",
      "Minibatch: 4700. Training loss: 0.9048964977264404\n",
      "Minibatch: 4800. Training loss: 0.8180844187736511\n",
      "Minibatch: 4900. Training loss: 0.8649299740791321\n",
      "Minibatch: 5000. Training loss: 0.8794530034065247\n",
      "Minibatch: 5100. Training loss: 0.8052783608436584\n",
      "Minibatch: 5200. Training loss: 0.7798627614974976\n",
      "Minibatch: 5300. Training loss: 0.8535895347595215\n",
      "Minibatch: 5400. Training loss: 0.8629411458969116\n",
      "Minibatch: 5500. Training loss: 0.9242172241210938\n",
      "Minibatch: 5600. Training loss: 0.8197521567344666\n",
      "Minibatch: 5700. Training loss: 0.8716219663619995\n",
      "Minibatch: 5800. Training loss: 0.8540198802947998\n",
      "Minibatch: 5900. Training loss: 0.7797620892524719\n",
      "Minibatch: 6000. Training loss: 0.9810878038406372\n",
      "Minibatch: 6100. Training loss: 0.9159034490585327\n",
      "Minibatch: 6200. Training loss: 0.7665480375289917\n",
      "Minibatch: 6300. Training loss: 0.8402895927429199\n",
      "Minibatch: 6400. Training loss: 0.8761833906173706\n",
      "Minibatch: 6500. Training loss: 0.7914760112762451\n",
      "Minibatch: 6600. Training loss: 0.8655328154563904\n",
      "Minibatch: 6700. Training loss: 0.918762743473053\n",
      "Minibatch: 6800. Training loss: 0.9518869519233704\n",
      "Minibatch: 6900. Training loss: 0.8572893738746643\n",
      "Minibatch: 7000. Training loss: 0.8035870790481567\n",
      "Minibatch: 7100. Training loss: 0.828696072101593\n",
      "Minibatch: 7200. Training loss: 0.8335672616958618\n",
      "Minibatch: 7300. Training loss: 0.8010303378105164\n",
      "Minibatch: 7400. Training loss: 0.784497082233429\n",
      "Minibatch: 7500. Training loss: 0.868496298789978\n",
      "Minibatch: 7600. Training loss: 0.9840120077133179\n",
      "Minibatch: 7700. Training loss: 0.8947007656097412\n",
      "Minibatch: 7800. Training loss: 0.8454426527023315\n",
      "Minibatch: 7900. Training loss: 0.8159546852111816\n",
      "Minibatch: 8000. Training loss: 0.8248254060745239\n",
      "Minibatch: 8100. Training loss: 0.9220570921897888\n",
      "Minibatch: 8200. Training loss: 0.874681830406189\n",
      "Minibatch: 8300. Training loss: 0.8260142803192139\n",
      "Minibatch: 8400. Training loss: 0.8869006037712097\n",
      "Minibatch: 8500. Training loss: 0.8922602534294128\n"
     ]
    }
   ],
   "source": [
    "# clip = 1.0\n",
    "epochs_number = 100\n",
    "sample_history = []\n",
    "best_val_loss = float(\"inf\")\n",
    "loss_list = []\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch_number in range(epochs_number):   \n",
    "\n",
    "    N = 100\n",
    "    \n",
    "    for minibatch_count, (input, target) in enumerate(tqdm_notebook(train_loader), 0):\n",
    "        model.train()\n",
    "        \n",
    "        input = input.float()\n",
    "        target = target.float()\n",
    "        input = input.to(computing_device)\n",
    "        target = target.to(computing_device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss_list.append(loss)\n",
    "        loss.backward()\n",
    "    \n",
    "        if minibatch_count % N == 0:\n",
    "            print('Minibatch: {}. Training loss: {}'.format(minibatch_count, loss))\n",
    "    \n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch_number % N == 0: \n",
    "        current_val_loss = validate(model, criterion_val, val_loader)\n",
    "        print('epoch: {}. Training loss: {}, Val loss: {}'.format(epoch_number, \n",
    "                                                                  loss,\n",
    "                                                                  current_val_loss))\n",
    "        val_loss_list.append(current_val_loss)\n",
    "    \n",
    "    else:\n",
    "        print('epoch: {}. Training loss: {}'.format(epoch_number, loss))\n",
    "    \n",
    "    \n",
    "    if current_val_loss < best_val_loss :        \n",
    "        torch.save(model.state_dict(), 'DAE.pt')\n",
    "        best_val_loss = current_val_loss\n",
    "        \n",
    "torch.save((loss_list, val_loss_list), './losses.pt')\n",
    "plt.figure()\n",
    "plt.plot(range(len(loss_list)), loss_list, label='Training Loss')\n",
    "plt.plot(range(0, len(val_loss_list) * N * minibatch_count, N * minibatch_count) , val_loss_list, label='Vadiation Loss')\n",
    "plt.xlabel('minibatches')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
